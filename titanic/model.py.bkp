import sys

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
import pandas as pd


def read_csv(filename):
    df = pd.read_csv(filename)
    df.set_index('PassengerId', inplace=True)
    return df


def format_cols(df):
    # TODO: Should use one-hot encoding for nominal variables
    df.Embarked.replace({'C': 0, 'Q': 1, 'S': 2, None: 0}, inplace=True)
    df.Fare.fillna(df.Fare.median(), inplace=True)
    df.Age.fillna(df.Age.median(), inplace=True)
    # children = df[df.Age < 18]
    # male_adult = df[(df.Sex == 'male') & (df.Age >= 18)]
    # female_adult = df[(df.Sex == 'female') & (df.Age >= 18)]
    # # Male children ages
    # mask = df.Name.str.contains('Master') & df.Age.isnull()
    # df.loc[mask, 'Age'] = df.loc[mask, 'Age'].fillna(children.Age.median())
    # # Male adult ages - need to come after filling children ages
    # mask = (df.Sex == 'male') & df.Age.isnull()
    # df.loc[mask, 'Age'] = df.loc[mask, 'Age'].fillna(male_adult.Age.median())
    # # Female ages
    # mask = (df.Sex == 'female') & df.Age.isnull()
    # df.loc[mask, 'Age'] = df.loc[mask, 'Age'].fillna(female_adult.Age.median())
    # Sex must come after ages
    df.Sex.replace({'female': 0, 'male': 1}, inplace=True)


def prepare_X_first(df):  # predicted_1 and predicted_2.csv
    format_cols(df)
    return df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]


def prepare_X_simple(df):
    # Hypothesis:
    # - Pclass and Fare are correlated
    # - Group size does not matter
    # - Embarked does not matter
    format_cols(df)
    df['Alone'] = (df['SibSp'] == 0) & (df['Parch'] == 0)
    X = df[['Pclass', 'Sex', 'Age', 'Alone']]
    return X


def prepare_y(df):
    return df['Survived']


def train(X, y):
    # TODO: Experiment with different max_leaf_nodes and max_depth
    clf = GradientBoostingClassifier(
        n_estimators=200, learning_rate=1.0, max_depth=1, random_state=0)\
        .fit(X_train, y_train)
    return clf


def xgboost():
    pass
    # TODO: Run with all data should use the n_estimators found in previous runs with early_stopping_rounds
    # XGBRegressor(n_estimators=500, learning_rate=0.05).fit(X_train, y_train, early_stopping_rounds=5)


def test(clf, X, y):
    score = clf.score(X, y)
    print('score: ', score)


def predict(clf, X):
    y = clf.predict(X)
    return pd.DataFrame(y, index=X.index, columns=['Survived'])


prepare_X = prepare_X_first

# TODO: Use scikit pipeline
df = read_csv('train.csv')
X = prepare_X(df)
y = prepare_y(df)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# TODO: Try xgboost.XGBRegressor and sklearn.metrics.mean_absolute_error
# https://www.kaggle.com/alexisbcook/xgboost
clf = train(X_train, y_train)

# TODO: Test with cross validation, check cross_val_score and different scoring
# https://www.kaggle.com/alexisbcook/cross-validation
# test(clf, X_test, y_test)

df = read_csv('test.csv')
X = prepare_X(df)
predicted = predict(clf, X)
predicted.to_csv(sys.stdout)
